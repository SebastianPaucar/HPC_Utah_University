{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Transfer Nodes for Large Data Transfer at Utah CHPC**\n",
    "For science workflows that transfer very large datasets between institutions, we need ***advanced parallel transfer tools*** running on tuned devices such as ***Data Transfer Nodes (DTNs)***. The University of Utah CHPC supports various parallel transfer tools that support these heavyweight tasks.\n",
    "\n",
    "**DTNs** are dedicated CHPC nodes desgined to handle high-speed large-data transfers. **DTNs** are optimized for moving heavy data volumes within the Utah CHPC network and between CHPC and external soruces (such as our local machine).\n",
    "\n",
    "Network traffic from most CHPC systems (on campus) pass through the campus firewall when communicating  with resources off campus.\n",
    "* Large research computing workflows require more bandwidth and connections/sessions requirements than the campus firewall can handle: it overwhelm the campus firewall capacity, impacting the usage for the rest of campos.\n",
    "    * For adress these needs, Utah campus has created a Science DMZ (a network segment with different security approaches) that allows for specific transfers (high performance and low latency) of data.\n",
    "## **General DTN environments**\n",
    "There are (all CHPC users are able to utilize the following):\n",
    "* intdtn01.chpc.utah.edu (connected at 10gbs, no dmz, use for internal campus transfers)\n",
    "* intdtn02.chpc.utah.edu (connected at 10gbs, no dmz, use for internal campus transfers)\n",
    "* intdtn03.chpc.utah.edu (connected at 10gbs, no dmz, use for internal campus transfers)\n",
    "* intdtn04.chpc.utah.edu (connected at 10gbs, no dmz, use for internal campus transfers)\n",
    "* dtn05.chpc.utah.edu (connected via dmz at 100gbs)\n",
    "* dtn06.chpc.utah.edu (connected via dmz at 100gbs)\n",
    "* dtn07.chpc.utah.edu (connected via dmz at 100gbs)\n",
    "* dtn08.chpc.utah.edu (connected via dmz at 100gbs)\n",
    "\n",
    "Where (for moving large datasets):\n",
    "* dtn05-08 operate individually, as well as together.\n",
    "* intdtn01-03 operate both individually as well as together.\n",
    "Furthermore:\n",
    "* CHPC supports specialized tools for moving data to/from cloud storage.\n",
    "    * `s3cmd` for Amazon cloud services\n",
    "    * `rclone` for different cloud storage providers.\n",
    "* **dtns** via slurm is enabled at `notchpeak`.\n",
    "# Data Transfer Node Access via SLURM\n",
    "It is good know that each **dtn** node has:\n",
    "* 24 cores, 128 GB RAM\n",
    "    * Only 12 cores and 96 GB RAM are avialable to run Slurm jobs.\n",
    "        * For `notchpeak` cluster:\n",
    "            * Slurm partition: `notchpeak-dtn`.\n",
    "            * Slurm Account: `dtn`.\n",
    "            * Nodes: `dtn05`,`dtn06`,`dtn07`,`dtn08`.\n",
    "            * `notchpeak-dtn` has 100 Gbps connections to the **Utah's Science DMZ** (segment of the Utah network with streamlined data-flow across the campus firewall to and from off-campus).\n",
    "    * All CHPC users have been set up to use the dtns.\n",
    "\n",
    "`notchpeak-dtn` Slurm partition is similar to other shared SLURM partitions at CHPC, with multiple transfer jobs sharing a node.\n",
    "* Each Slurm job running on a **dtn** is allocated a 1 core and 2 GB RAM.\n",
    "* `notchpeak-dtn` has 72 hours per job as a maximum limit time.\n",
    "* For parallel transfers, users can request the number of cores and memory using `$SBATCH` directives.\n",
    "## **Download a dataset using dtn Slurm script:**\n",
    "```bash\n",
    "#!/bin/tcsh \n",
    "\n",
    "#SBATCH --partition=notchpeak-dtn\n",
    "\n",
    "#SBATCH --account=dtn\n",
    "\n",
    "#SBATCH --time=1:00:00\n",
    "\n",
    "#SBATCH -o slurm-%j.out-%N\n",
    "\n",
    "#SBATCH -e slurm-%j.err-%N s\n",
    "\n",
    "setenv SCR /scratch/general/lustre/$USER/$SLURM_JOB_ ID\n",
    "\n",
    "mkdir -p $SCR\n",
    "\n",
    "cd $SCR\n",
    "\n",
    "wget https://www1.ncdc.noaa.gov/pub/data/uscrn/products/daily01/2020/CRND0103-2020-AK_Aleknagik_1_NNE.txt\n",
    "```\n",
    "Note that:\n",
    "* The appropiate account and partition were used (`notchpeak-dtn` and `dtn`).\n",
    "* `setenv SCR /scratch/general/lustre/$USER/$SLURM_JOB_ID` set an environment variable `SCR` to a path in the scratch file system (specific to the user and job ID). `setenv` (`tcsh` command) is equivalent to `export` in bash. \n",
    "    * `SCR` is the name of the environment variable being set.\n",
    "    * `/scratch/general/lustre` is a directory path on the file system intended for temporary or intermediate data storage. The **scratch** space is a HP temporary storage area.\n",
    "    * `$USER`is an environment variable that ensures each user's data is kept separate.\n",
    "    * `$SLURM_JOB_ID` is an Slurm environment variable, containing the unique job ID assigned to the current job. It ensures that data from different jobs run by the same user doesn't collide and is stored in separate directories.\n",
    "        * `/scratch/general/lustre/$USER/$SLURM_JOB_ID` is the value being assigned to the `SCR` environment variable. It constructs a path where temporary files can be stored for the job.\n",
    "* `mkdir -p $SCR` creates the directory if it doesn't already exist, ensuring that the path `scratch/general/lustre/$USER/$SLURM_JOB_ID` exists. \n",
    "* `cd $SCR` changes the current directory to the one just created.\n",
    "* `wget [...]` downloads a specific file into the directory defined by `SCR`.\n",
    "## **There is a problem with `/scratch/general/lustre/`!**\n",
    "Note that you have to join to the `notchpeak` cluster to use the `notchpeak-dtn` partition and `dtn` account. Now, let's see this:\n",
    "```bash\n",
    "(base) [u6059911@notchpeak2:~]$ cd /scratch/general/lustre/ && mkdir $USER\n",
    "mkdir: cannot create directory ‘u6059911’: Read-only file system\n",
    "```\n",
    "It means that `/scratch/general/lustre/` is a Read-only file system, for this reason the following commands (that were used in the Slurm script above):\n",
    "```bash\n",
    "setenv SCR /scratch/general/lustre/$USER/$SLURM_JOB_ID\n",
    "\n",
    "mkdir -p $SCR\n",
    "\n",
    "cd $SCR\n",
    "```\n",
    "Don't have sense, because we are not able to create directories inside `lustre`. If we try run the Slurm bath script using the `scratch` path, despite what was mentioned above, we will find:\n",
    "```bash\n",
    "(base) [u6059911@notchpeak2:~]$ cat slurm-1458728.out-dtn05 \n",
    "(base) [u6059911@notchpeak2:~]$ cat slurm-1458728.err-dtn05 \n",
    "mkdir: cannot create directory ‘/scratch/general/lustre/u6059911’: Permission denied\n",
    "/scratch/general/lustre/u6059911/1458728: No such file or directory.\n",
    "```\n",
    "And the Slurm batch job for data transfer will not have been executed. To overcome this problem, we need to change the job directory path. \n",
    "    * Use `\\tmp` could be good, because it often is in fast storage, has broad write permissions, and the files are automatically deleted after a while.\n",
    "```bash\n",
    "#!/bin/tcsh\n",
    "\n",
    "#SBATCH --partition=notchpeak-dtn         \n",
    "#SBATCH --account=dtn                      \n",
    "#SBATCH --time=1:00:00                    \n",
    "#SBATCH --ntasks=1                       \n",
    "#SBATCH --cpus-per-task=1                \n",
    "#SBATCH --mem=4GB                        \n",
    "#SBATCH -o slurm-%j.out-%N               \n",
    "#SBATCH -e slurm-%j.err-%N             \n",
    "\n",
    "setenv SCR /tmp/$USER/$SLURM_JOB_ID\n",
    "\n",
    "mkdir -p $SCR\n",
    "\n",
    "cd $SCR\n",
    "\n",
    "echo \"Working directory: $SCR\"\n",
    "\n",
    "wget https://www1.ncdc.noaa.gov/pub/data/uscrn/products/daily01/2020/CRND0103-2020-AK_Aleknagik_1_NNE.txt\n",
    "\n",
    "ls -lh CRND0103-2020-AK_Aleknagik_1_NNE.txt\n",
    "\n",
    "head CRND0103-2020-AK_Aleknagik_1_NNE.txt\n",
    "```\n",
    "Then, you can submit the Slurm batch job using `sbatch` and the `.slurm` file name that was executed.\n",
    "```bash\n",
    "(base) [u6059911@notchpeak2:~]$ sbatch dtn_test.slurm \n",
    "Submitted batch job 1458720\n",
    "```\n",
    "Note that how the job is beingf processed:\n",
    "```bash\n",
    "(base) [u6059911@notchpeak2:~]$ squeue -u $USER\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "           1458720 notchpeak dtn_test u6059911 CG       0:05      1 dtn05\n",
    "(base) [u6059911@notchpeak2:~]$ squeue -u $USER\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "```\n",
    "Then you can see the `.err` & `.out` outputs of the Slurm job:\n",
    "```bash\n",
    "(base) [u6059911@notchpeak2:~]$ ls\n",
    "dtn_test.slurm  Miniconda3-latest-Linux-x86_64.sh  scripts                  slurm-1458720.out-dtn05\n",
    "environments    MyModules                          slurm-1458720.err-dtn05  software\n",
    "(base) [u6059911@notchpeak2:~]$ cat sl\n",
    "slurm-1458720.err-dtn05  slurm-1458720.out-dtn05\n",
    "```\n",
    "Thus, you can see what the `.out` file contains using `cat`.   \n",
    "```bash\n",
    "(base) [u6059911@notchpeak2:~]$ cat slurm-1458720.out-dtn05 \n",
    "Working directory: /tmp/u6059911/1458720\n",
    "-rw-r--r-- 1 u6059911 nineil 78K Nov 20  2022 CRND0103-2020-AK_Aleknagik_1_NNE.txt\n",
    "23583 20200101  2.514 -158.61   59.28   -19.0   -27.8   -23.4   -21.0     0.0     0.31 C   -20.0   -36.2   -25.2    79.3    49.9    62.6 -99.000 -99.000 -99.000 -99.000 -99.000 -9999.0 -9999.0 -9999.0 -9999.0 -9999.0\n",
    "23583 20200102  2.514 -158.61   59.28   -21.8   -28.0   -24.9   -24.1     0.0     0.90 C   -25.7   -36.4   -29.7    86.6    64.2    73.5 -99.000 -99.000 -99.000 -99.000 -99.000 -9999.0 -9999.0 -9999.0 -9999.0 -9999.0\n",
    "23583 20200103  2.514 -158.61   59.28   -17.7   -22.8   -20.2   -20.2     0.0     1.22 C   -21.9   -27.1   -24.6    71.5    59.9    64.6 -99.000 -99.000 -99.000 -99.000 -99.000 -9999.0 -9999.0 -9999.0 -9999.0 -9999.0\n",
    "23583 20200104  2.514 -158.61   59.28   -20.1   -26.0   -23.0   -22.3     0.0     1.19 C   -18.1   -32.2   -26.0    89.5    69.3    78.1 -99.000 -99.000 -99.000 -99.000 -99.000 -9999.0 -9999.0 -9999.0 -9999.0 -9999.0\n",
    "```\n",
    "What verifies that the dataset has been downloaded correctly. Note that it will be deleted after a while (because the `\\tmp` nature). If you want that the dataset to stay in a directory, you will have to change the path that follows `setenv`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
