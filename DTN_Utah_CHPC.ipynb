{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Transfer Nodes for Large Data Transfer at Utah CHPC**\n",
    "For science workflows that transfer very large datasets between institutions, we need ***advanced parallel transfer tools*** running on tuned devices such as ***Data Transfer Nodes (DTNs)***. The University of Utah CHPC supports various parallel transfer tools that support these heavyweight tasks.\n",
    "\n",
    "Network traffic from most CHPC systems (on campus) pass through the campus firewall when communicating  with resources off campus.\n",
    "* Large research computing workflows require more bandwidth and connections/sessions requirements than the campus firewall can handle: it overwhelm the campus firewall capacity, impacting the usage for the rest of campos.\n",
    "    * For adress these needs, Utah campus has created a Science DMZ (a network segment with different security approaches) that allows for specific transfers (high performance and low latency) of data.\n",
    "## **General DTN environments**\n",
    "There are (all CHPC users are able to utilize the following):\n",
    "* intdtn01.chpc.utah.edu (connected at 10gbs, no dmz, use for internal campus transfers)\n",
    "* intdtn02.chpc.utah.edu (connected at 10gbs, no dmz, use for internal campus transfers)\n",
    "* intdtn03.chpc.utah.edu (connected at 10gbs, no dmz, use for internal campus transfers)\n",
    "* intdtn04.chpc.utah.edu (connected at 10gbs, no dmz, use for internal campus transfers)\n",
    "* dtn05.chpc.utah.edu (connected via dmz at 100gbs)\n",
    "* dtn06.chpc.utah.edu (connected via dmz at 100gbs)\n",
    "* dtn07.chpc.utah.edu (connected via dmz at 100gbs)\n",
    "* dtn08.chpc.utah.edu (connected via dmz at 100gbs)\n",
    "\n",
    "Where (for moving large datasets):\n",
    "* dtn05-08 operate individually, as well as together.\n",
    "* intdtn01-03 operate both individually as well as together.\n",
    "Furthermore:\n",
    "* CHPC supports specialized tools for moving data to/from cloud storage.\n",
    "    * `s3cmd` for Amazon cloud services\n",
    "    * `rclone` for different cloud storage providers.\n",
    "* **dtns** via slurm is enabled at `notchpeak`.\n",
    "# Data Transfer Node Access via SLURM\n",
    "It is good know that each **dtn** node has:\n",
    "* 24 cores, 128 GB RAM\n",
    "    * Only 12 cores and 96 GB RAM are avialable to run Slurm jobs.\n",
    "        * For `notchpeak` cluster:\n",
    "            * Slurm partition: `notchpeak-dtn`.\n",
    "            * Slurm Account: `dtn`.\n",
    "            * Nodes: `dtn05`,`dtn06`,`dtn07`,`dtn08`.\n",
    "            * `notchpeak-dtn` has 100 Gbps connections to the **Utah's Science DMZ** (segment of the Utah network with streamlined data-flow across the campus firewall to and from off-campus).\n",
    "    * All CHPC users have been set up to use the dtns.\n",
    "\n",
    "`notchpeak-dtn` Slurm partition is similar to other shared SLURM partitions at CHPC, with multiple transfer jobs sharing a node.\n",
    "* Each Slurm job running on a **dtn** is allocated a 1 core and 2 GB RAM.\n",
    "* `notchpeak-dtn` has 72 hours per job as a maximum limit time.\n",
    "\n",
    "## **Download a dataset using dtn Slurm script:**\n",
    "```bash\n",
    "#!/bin/tcsh \n",
    "\n",
    "#SBATCH --partition=notchpeak-dtn\n",
    "\n",
    "#SBATCH --account=dtn\n",
    "\n",
    "#SBATCH --time=1:00:00\n",
    "\n",
    "#SBATCH -o slurm-%j.out-%N\n",
    "\n",
    "#SBATCH -e slurm-%j.err-%N s\n",
    "\n",
    "setenv SCR /scratch/general/lustre/$USER/$SLURM_JOB_ ID\n",
    "\n",
    "mkdir -p $SCR\n",
    "\n",
    "cd $SCR\n",
    "\n",
    "wget https://www1.ncdc.noaa.gov/pub/data/uscrn/products/daily01/2020/CRND0103-2020-AK_Aleknagik_1_NNE.txt\n",
    "```\n",
    "Note that:\n",
    "* The appropiate account and partition were used (`notchpeak-dtn` and `dtn`).\n",
    "* `setenv SCR /scratch/general/lustre/$USER/$SLURM_JOB_ID` set an environment variable `SCR` to a path in the scratch file system (specific to the user and job ID). `setenv` (`tcsh` command) is equivalent to `export` in bash. \n",
    "    * `SCR` is the name of the environment variable being set.\n",
    "    * `/scratch/general/lustre` is a directory path on the file system intended for temporary or intermediate data storage. The **scratch** space is a HP temporary storage area.\n",
    "    * `$USER`is an environment variable that ensures each user's data is kept separate.\n",
    "    * `$SLURM_JOB_ID` is an Slurm environment variable, containing the unique job ID assigned to the current job. It ensures that data from different jobs run by the same user doesn't collide and is stored in separate directories.\n",
    "        * `/scratch/general/lustre/$USER/$SLURM_JOB_ID` is the value being assigned to the `SCR` environment variable. It constructs a path where temporary files can be stored for the job.\n",
    "* `mkdir -p $SCR` creates the directory if it doesn't already exist, ensuring that the path `scratch/general/lustre/$USER/$SLURM_JOB_ID` exists. \n",
    "* `cd $SCR` changes the current directory to the one just created.\n",
    "* `wget [...]` downloads a specific file into the directory defined by `SCR`.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
